<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>linear-regression - Tag -</title><link>https://youngeun-in.github.io/tags/linear-regression/</link><description>linear-regression - Tag -</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 27 Mar 2022 15:12:26 +0900</lastBuildDate><atom:link href="https://youngeun-in.github.io/tags/linear-regression/" rel="self" type="application/rss+xml"/><item><title>선형회귀</title><link>https://youngeun-in.github.io/%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80/</link><pubDate>Sun, 27 Mar 2022 15:12:26 +0900</pubDate><author>Author</author><guid>https://youngeun-in.github.io/%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80/</guid><description>선형 회귀는 한 개 이상의 독립 변수 x와 y의 선형 관계를 모델링한다.
가중치 행렬을 W, 편향을 b, 실제값을 y라고 할 때, 선형회귀는 비용함수 MSE를 최소화하는 W와 b를 추정해 나가는 과정이다.
계산 과정 편차를 $WX_{i}+b-y_{i}$라고 할 때, 비용함수는 다음과 같다.
$$\sum_{i=1}^{m} (WX_{i}+b-y_{i})^2 = \sum_{i=1}^{m}(X_{i}^2W^2+2X_{i}bW-2by_{i}-2X_{i}y_{i}W+b^2+yi^2)$$
비용함수를 W로 각각 편미분 하면 다음과 같다.
$$\partial{W} = \cfrac{\partial{cost(W,b)}} {\partial{W}} = \sum_{i=1}^{m}(2X_{i}^2W+2X_{i}b-2X_{i}y_{i})\cdot \cfrac{1}{m} $$
$$= 2X_{i}\sum_{i=1}^{m} (WX_{i}+b-y_{i})\cdot \cfrac{1}{m}$$
비용함수를 b로 각각 편미분 하면 다음과 같다.
$$\partial{b} = \cfrac{\partial{cost(W,b)}} {\partial{b}} = \sum_{i=1}^{m}(2X_{i}W-2y_{i}+2b)\cdot \cfrac{1}{m}$$</description></item></channel></rss>