---
title: 활성화 함수
date: 2022-01-11T15:12:26+09:00
categories:
  - ML
tags: 
  - step-function
  - sigmoid
  - relu
---

##  Step Function

활성화 함수는 신경망의 행동을 결정하는 중요한 역할을 합니다. 가장 간단한 형태의 활성화 함수는 **계단 함수(Step function)** 라고 합니다. 계단 함수는 입력값의 합이 임계값을 넘으면 $0$ 을, 넘지 못하면 $1$ 을 출력하게 됩니다.
계단 함수의 그래프는 다음과 같이 생겼습니다.

<p align="center"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Dirac_distribution_CDF.svg/1280px-Dirac_distribution_CDF.svg.png" alt="step_function" style="zoom: 33%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://en.wikipedia.org/wiki/Heaviside_step_function">wikipedia - Heaviside step function</a></p>

계단 함수는 활성화 함수의 조건을 가장 잘 만족하는 함수이고 직관적으로도 이해하기 쉽습니다. 하지만 **불연속 함수**라는 단점 때문에 실제 신경망에 사용되지는 않습니다. 
그래프를 보면 알 수 있듯 임계값 지점에서 불연속점을 갖게 되는데 이 점에서 미분이 불가능하기 때문에 학습이 필요한 신경망에 사용할 수 없습니다.
이런 문제점을 해결하기 위해서 등장한 것이 **시그모이드 함수(Sigmoid)** 입니다.

## Sigmoid Function

시그모이드 함수는 기본적으로 $S$ 모양을 그리는 곡선 함수를 통칭하여 부르는 말입니다. 이 중 대표적인 함수는 **로지스틱(Logistic) 함수**와 **하이퍼탄젠트(Hyper tangent, $\tanh$) 함수**가 있습니다. 두 함수의 수식과 그래프를 보며 시그모이드 함수와 계단 함수가 다른 점이 무엇인지 알아보도록 하겠습니다.

### 로지스틱 함수(Logistic Function)


$$
\text{Logistic} : \frac{1}{1+e^{-x}}
$$

<p align="center"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1920px-Logistic-curve.svg.png" alt="logistic" style="zoom: 25%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://en.wikipedia.org/wiki/Logistic_function">wikipedia - Logistic function</a></p>

### 하이퍼탄젠트 함수(Hypertangent Function)


$$
\text{Hypertangent} : \frac{e^x-e^{-x}}{e^x+e^{-x}} = \frac{e^{2x}-1}{e^{2x}+1}
$$


<p align="center"><img src="https://mathworld.wolfram.com/images/interactive/TanhReal.gif" alt="hyper" style="zoom:110%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://mathworld.wolfram.com/HyperbolicTangent.html">mathworld.wolfram.com</a></p>

두 함수는 모두 연속함수입니다. 계단 함수의 치명적인 단점이었던 불연속을 해결했지요. 계단 함수와 시그모이드 함수의 중요한 공통점은 **비선형 함수(Non-linear)** 라는 점입니다.

활성화 함수는 비선형 함수를 사용해야 합니다. 활성화 함수가 선형 함수이면 안되는 이유는 무엇일까요? 선형인 활성화 함수 $l(x) = ax + b$ 가 있다고 해보겠습니다. 이 함수를 사용하여 3개의 층을 쌓는다면 최종적인 활성화 함수는 $l(l(l(x))) = l^3(x) = a(a(ax+b)+b)+b = a^3x+a^2b+ab+b$가 됩니다. $a^3 = c, d = a^2b+ab+b$라고 하면 $l^3(x) = cx+d$로 여전히 같은 형태의 함수를 사용하게 됩니다. 이렇듯 층을 아무리 깊게 쌓아도 여러 층을 쌓는 이점을 살리지 못하게 되지요. 여러 층을 쌓을 때의 장점을 살리기 위해 비선형 함수를 사용하게 되는 것이지요.


## ReLU Function

시그모이드 함수는 불연속이라는 계단 함수의 단점을 해결했습니다. 하지만 시그모이드는 대부분의 점에서 기울기 값이 0이 됩니다. 이 때문에 **기울기 소실(Gradient vanishing)**이라는 문제가 발생합니다. 기울기 소실은 시그모이드 함수를 활성화 함수로 사용하여 층을 깊게 쌓았을 때 학습이 잘 되지 않는 현상입니다. 이런 현상이 왜 발생하는지 알아보겠습니다.

로지스틱 함수 $L(x)$를 미분한 함수 $L^\prime(x)$ 의 수식은 다음과 같습니다.


$$
L^\prime(x) = \bigg(\frac{1}{1+e^{-x}}\bigg)^\prime = \frac{e^x}{(1+e^{-x})^2}
$$


위 함수의 그래프는 아래와 같이 생겼습니다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/91560184-5d5bd900-e974-11ea-8c02-2a182c6a7c93.png" alt="logistic_deri" style="zoom:67%;" /></p>

그래프에서 볼 수 있듯 최댓값이 $0.25$ 밖에 되지 않고 $x<-5, x>5$ 범위에서는 거의 $0$ 에 가깝습니다. [역전파(Back propagation)](https://yngie-c.github.io/deep%20learning/2020/03/14/back_propagation/) 과정에서는 미분값을 사용하여 학습을 하게 됩니다. 따라서 이 값이 0에 가까워 지면 정보가 유실되면서 학습이 잘 안되게 됩니다. 특히 층을 깊게 쌓을 경우에는 정보가 모두 유실되는 사태가 발생하게 되지요.

그렇다면 하이퍼탄젠트 함수는 어떻게 될까요? 하이퍼탄젠트 함수 $\tanh$를 미분한 함수의 수식은 다음과 같습니다.

$$
\tanh^\prime(x) = \bigg(\frac{e^x-e^{-x}}{e^x+e^{-x}}\bigg)^\prime = \frac{4e^{2x}}{(1+e^{2x})^2}
$$


위 함수의 그래프는 아래와 같이 생겼습니다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/91560164-52a14400-e974-11ea-8bf4-bbfc7fd42deb.png" alt="hypertangent_deri" style="zoom: 67%;" /></p>

하이퍼탄젠트 함수를 미분한 함수의 최댓값은 $1$ 입니다. 최댓값이 $0.25$ 밖에 안되었던 로지스틱 함수 보다는 정보를 잘 전달하게 되지요. 하지만 여전히 $x$ 가 0에서 멀어질수록 원래 함수의 미분값은 0에 가까워집니다. 그래서 하이퍼탄젠트 함수를 활성화 함수로 하더라도 퍼셉트론을 여러 층으로 쌓는다면 학습이 제대로 안되게 되지요. 이렇게 시그모이드 함수를 활성화 함수로 사용할 때 역전파시 학습이 제대로 진행되지 않는 현상을 **기울기 소실**이라고 합니다.

기울기 소실 문제를 극복하기 위해서 등장한 함수가 바로 **ReLU(Rectified Linear Unit)함수**입니다. ReLU함수는 입력값이 0보다 작을 경우에는 0을 반환하고, 0보다 클 경우에는 입력값을 그대로 반환합니다.

아래는 ReLU함수의 그래프를 나타낸 것입니다.

<p align="center"><img src="https://miro.medium.com/max/1225/0*g9ypL5M3k-f7EW85.png" alt="ReLU" style="zoom:50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://medium.com/@sonish.sivarajkumar/relu-most-popular-activation-function-for-deep-neural-networks-10160af37dda">medium.com</a></p>

ReLU함수는 $x$ 가 $0$ 보다 클 때, 미분값이 항상 $1$ 입니다. 그래서 층이 아무리 깊어져도 손실없이 정보를 전달할 수 있습니다. 미분값이 항상 $0$과 $1$ 이기 때문에 연산이 빠르다는 점도 ReLU함수의 장점입니다. 덕분에 ReLU함수는 은닉층에서 가장 많이 사용되는 활성화 함수가 되었습니다. 물론 ReLU함수에게도 문제가 있습니다. 0이하의 값이 그대로 보존되지 않고 버려진다는 것이지요. 이를 보완하기 위해 **Leaky ReLU**함수가 고안되어 사용되고 있습니다.

일반적으로는 $a=0.01$을 사용하며 그래프는 다음과 같습니다.

<p align="center"><img src="https://miro.medium.com/max/1225/1*siH_yCvYJ9rqWSUYeDBiRA.png" alt="leaky" style="zoom:50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://medium.com/@himanshuxd/activation-functions-sigmoid-relu-leaky-relu-and-softmax-basics-for-neural-networks-and-deep-8d9c70eed91e">medium.com</a></p>
